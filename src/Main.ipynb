{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>URL</th>\n",
       "      <th>URLLength</th>\n",
       "      <th>Domain</th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>TLD</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>...</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Crypto</th>\n",
       "      <th>HasCopyrightInfo</th>\n",
       "      <th>NoOfImage</th>\n",
       "      <th>NoOfCSS</th>\n",
       "      <th>NoOfJS</th>\n",
       "      <th>NoOfSelfRef</th>\n",
       "      <th>NoOfEmptyRef</th>\n",
       "      <th>NoOfExternalRef</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.northcm.ac.th</td>\n",
       "      <td>24.0</td>\n",
       "      <td>www.northcm.ac.th</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>8135291.txt</td>\n",
       "      <td>http://uqr.to/1il1z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>586561.txt</td>\n",
       "      <td>https://www.woolworthsrewards.com.au</td>\n",
       "      <td>35.0</td>\n",
       "      <td>www.woolworthsrewards.com.au</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>au</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.522907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>412632.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.nyprowrestling.com</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     FILENAME                                   URL  URLLength  \\\n",
       "0   1          NaN             https://www.northcm.ac.th       24.0   \n",
       "1   4  8135291.txt                   http://uqr.to/1il1z        NaN   \n",
       "2   5   586561.txt  https://www.woolworthsrewards.com.au       35.0   \n",
       "3   6          NaN                                   NaN       31.0   \n",
       "4  11   412632.txt                                   NaN        NaN   \n",
       "\n",
       "                         Domain  DomainLength  IsDomainIP  TLD  \\\n",
       "0             www.northcm.ac.th          17.0         0.0  NaN   \n",
       "1                           NaN           NaN         NaN   to   \n",
       "2  www.woolworthsrewards.com.au          28.0         0.0   au   \n",
       "3                           NaN           NaN         NaN  com   \n",
       "4        www.nyprowrestling.com          22.0         0.0  NaN   \n",
       "\n",
       "   CharContinuationRate  TLDLegitimateProb  ...  Pay  Crypto  \\\n",
       "0              0.800000                NaN  ...  0.0     0.0   \n",
       "1              1.000000           0.000896  ...  NaN     0.0   \n",
       "2              0.857143                NaN  ...  1.0     0.0   \n",
       "3              0.562500           0.522907  ...  1.0     0.0   \n",
       "4              1.000000                NaN  ...  0.0     0.0   \n",
       "\n",
       "   HasCopyrightInfo  NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  \\\n",
       "0               1.0        NaN      3.0     NaN         69.0           NaN   \n",
       "1               0.0        NaN      NaN     NaN          NaN           NaN   \n",
       "2               1.0       33.0      7.0     8.0         15.0           NaN   \n",
       "3               1.0       24.0      5.0    14.0          NaN           NaN   \n",
       "4               1.0        NaN      NaN    14.0          NaN           0.0   \n",
       "\n",
       "   NoOfExternalRef  label  \n",
       "0              NaN      1  \n",
       "1              1.0      0  \n",
       "2              2.0      1  \n",
       "3              NaN      1  \n",
       "4              NaN      1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://drive.google.com/uc?id=1w8DnxXGwaF1dLKlMxmXWVQFOH4vcvfTp')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Categorical Features: 22\n",
      "Categorical Features: ['FILENAME', 'URL', 'Domain', 'TLD', 'Title', 'IsDomainIP', 'HasObfuscation', 'IsHTTPS', 'HasTitle', 'HasFavicon', 'IsResponsive', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo', 'Robots', 'HasDescription']\n",
      "Jumlah Non-Categorical Features: 34\n",
      "Non-Categorical Features: ['id', 'URLLength', 'DomainLength', 'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength', 'NoOfSubDomain', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'LineOfCode', 'LargestLineLength', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'NoOfPopup', 'NoOfiFrame', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef', 'label']\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "#Categorical bersifat numeric diambil dari metadata yang ada di artikel https://www.sciencedirect.com/science/article/pii/S0167404823004558#se0060\n",
    "numerical_categorical_columns = [\n",
    "    \"IsDomainIP\",         #1/0\n",
    "    \"HasObfuscation\",     #1/0\n",
    "    \"IsHTTPS\",            #1/0\n",
    "    \"HasTitle\",           #1/0\n",
    "    \"HasFavicon\",         #1/0\n",
    "    \"IsResponsive\",       #1/0\n",
    "    \"HasExternalFormSubmit\",  #1/0\n",
    "    \"HasSocialNet\",       #1/0\n",
    "    \"HasSubmitButton\",    #1/0\n",
    "    \"HasHiddenFields\",    #1/0\n",
    "    \"HasPasswordField\",   #1/0\n",
    "    \"Bank\",               #1/0\n",
    "    \"Pay\",                #1/0\n",
    "    \"Crypto\",             #1/0\n",
    "    \"HasCopyrightInfo\",   #1/0\n",
    "    \"Robots\",             #1/0, asumsi bahwa 1/0 merupakan ada atau tidaknya robot karena tidak ada di metadata\n",
    "    \"HasDescription\",     #1/0\n",
    "]\n",
    "\n",
    "# Gabungkan semua kolom kategori\n",
    "categorical_columns += numerical_categorical_columns\n",
    "\n",
    "nonCategorical_columns = [col for col in df.columns if col not in categorical_columns]\n",
    "nonCategorical_df = pd.DataFrame(nonCategorical_columns)\n",
    "\n",
    "categorical_df = pd.DataFrame(categorical_columns)\n",
    "\n",
    "print(\"Jumlah Categorical Features:\", len(categorical_columns))\n",
    "print(\"Categorical Features:\", categorical_columns)\n",
    "print(\"Jumlah Non-Categorical Features:\", len(nonCategorical_columns))\n",
    "print(\"Non-Categorical Features:\", nonCategorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_columns, numerical_columns, missing_threshold=70, correlation_threshold=0.1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        categorical_columns : list\n",
    "            List nama kolom kategorikal\n",
    "        numerical_columns : list\n",
    "            List nama kolom numerik\n",
    "        missing_threshold : float, default=70\n",
    "            Persentase missing values yang diperbolehkan (kolom akan dihapus jika melebihi)\n",
    "        correlation_threshold : float, default=0.1\n",
    "            Minimum korelasi dengan target yang dibutuhkan (kolom akan dihapus jika kurang dari ini)\n",
    "        \"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.columns_to_drop = None\n",
    "        self.feature_scores = None\n",
    "    \n",
    "    def calculate_correlation_with_label(self, X, column, y):\n",
    "        \"\"\"Menghitung korelasi antara fitur dan label\"\"\"\n",
    "        try:\n",
    "            if column in self.categorical_columns:\n",
    "                # Spearman correlation untuk kolom kategorikal\n",
    "                try:\n",
    "                    col_data = pd.to_numeric(X[column], errors='coerce')\n",
    "                    correlation, _ = stats.spearmanr(col_data, y, nan_policy='omit')\n",
    "                    return abs(correlation) if not np.isnan(correlation) else 0\n",
    "                except:\n",
    "                    return 0\n",
    "            else:\n",
    "                # Pearson correlation untuk kolom numerik\n",
    "                correlation = abs(X[column].corr(y))\n",
    "                return correlation if not np.isnan(correlation) else 0\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate correlation for {column}: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def analyze_features(self, X, y):\n",
    "        \"\"\"Menganalisis fitur berdasarkan missing values dan korelasi dengan target\"\"\"\n",
    "        # Hitung persentase missing values\n",
    "        missing_percentages = (X.isnull().sum() / len(X)) * 100\n",
    "        \n",
    "        results = []\n",
    "        for column in X.columns:\n",
    "            if column == 'label':\n",
    "                continue\n",
    "                \n",
    "            correlation = self.calculate_correlation_with_label(X, column, y)\n",
    "            missing_pct = missing_percentages[column]\n",
    "            unique_count = X[column].nunique()\n",
    "            \n",
    "            results.append({\n",
    "                'column': column,\n",
    "                'missing_percentage': missing_pct,\n",
    "                'correlation_with_label': correlation,\n",
    "                'unique_values': unique_count,\n",
    "                'is_categorical': column in self.categorical_columns,\n",
    "                'score': missing_pct - (correlation * 100)  # Score untuk penentuan drop\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Menganalisis fitur dan menentukan kolom mana yang akan dihapus.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "        y : pandas Series atau numpy array\n",
    "            Target variable\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            raise ValueError(\"Target variable (y) harus disediakan untuk feature selection\")\n",
    "        \n",
    "        # Analisis fitur\n",
    "        self.feature_scores = self.analyze_features(X, y)\n",
    "        \n",
    "        # Identifikasi kolom yang akan dihapus\n",
    "        self.columns_to_drop = self.feature_scores[\n",
    "            (self.feature_scores['missing_percentage'] > self.missing_threshold) |\n",
    "            (self.feature_scores['correlation_with_label'] < self.correlation_threshold)\n",
    "        ]['column'].tolist()\n",
    "\n",
    "        self._print_summary(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Menghapus kolom yang telah diidentifikasi untuk dihapus.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas DataFrame\n",
    "            Data dengan kolom yang sudah dihapus\n",
    "        \"\"\"\n",
    "        if self.columns_to_drop is None:\n",
    "            raise ValueError(\"Transformer belum di-fit. Panggil fit() terlebih dahulu.\")\n",
    "        \n",
    "        return X.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "    \n",
    "    def _print_summary(self, X):\n",
    "        \"\"\"Mencetak ringkasan hasil feature selection\"\"\"\n",
    "        print(\"\\nFeature Selection Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total features awal: {X.shape[1]}\")\n",
    "        print(f\"Features yang akan dihapus: {len(self.columns_to_drop)}\")\n",
    "        print(f\"Features yang dipertahankan: {X.shape[1] - len(self.columns_to_drop)}\")\n",
    "        \n",
    "        if len(self.columns_to_drop) > 0:\n",
    "            print(\"\\nFeatures yang dihapus:\")\n",
    "            for col in self.columns_to_drop:\n",
    "                feat_info = self.feature_scores[self.feature_scores['column'] == col].iloc[0]\n",
    "                print(f\"\\n- {col}\")\n",
    "                print(f\"  Missing values: {feat_info['missing_percentage']:.2f}%\")\n",
    "                print(f\"  Correlation with target: {feat_info['correlation_with_label']:.4f}\")\n",
    "        \n",
    "        # Top features berdasarkan korelasi\n",
    "        print(\"\\nTop 5 features berdasarkan korelasi dengan target:\")\n",
    "        top_features = self.feature_scores.nsmallest(5, 'score')\n",
    "        for _, feat in top_features.iterrows():\n",
    "            print(f\"- {feat['column']}: correlation = {feat['correlation_with_label']:.4f}, \"\n",
    "                  f\"missing = {feat['missing_percentage']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_columns, numerical_columns, strategy='advanced'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        categorical_columns : list\n",
    "            List nama kolom kategorikal\n",
    "        numerical_columns : list\n",
    "            List nama kolom numerik\n",
    "        strategy : str, default='advanced'\n",
    "            Strategi untuk mengisi missing values:\n",
    "            - 'simple': mean untuk numerik, mode untuk kategorikal\n",
    "            - 'advanced': median untuk numerik yang skewed, mean untuk yang normal,\n",
    "                         mode untuk kategorikal dengan kardinalitas rendah,\n",
    "                         'MISSING' untuk kategorikal dengan kardinalitas tinggi\n",
    "        \"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.strategy = strategy\n",
    "        self.imputer_dict = {}\n",
    "        \n",
    "    def analyze_missing(self, X):\n",
    "        \"\"\"Menganalisis pola missing values pada dataset\"\"\"\n",
    "        # Hitung persentase missing values per kolom\n",
    "        missing_percentages = (X.isnull().sum() / len(X)) * 100\n",
    "        \n",
    "        missing_info = []\n",
    "        for col in X.columns:\n",
    "            if missing_percentages[col] > 0:\n",
    "                info = {\n",
    "                    'column': col,\n",
    "                    'missing_percentage': missing_percentages[col],\n",
    "                    'data_type': X[col].dtype,\n",
    "                }\n",
    "                \n",
    "                if col in self.numerical_columns:\n",
    "                    non_null_data = X[col].dropna()\n",
    "                    info.update({\n",
    "                        'mean': non_null_data.mean(),\n",
    "                        'median': non_null_data.median(),\n",
    "                        'skewness': non_null_data.skew()\n",
    "                    })\n",
    "                elif col in self.categorical_columns:\n",
    "                    info.update({\n",
    "                        'mode': X[col].mode().iloc[0] if not X[col].mode().empty else 'No mode',\n",
    "                        'unique_values': X[col].nunique(),\n",
    "                        'cardinality_ratio': X[col].nunique() / len(X)\n",
    "                    })\n",
    "                \n",
    "                missing_info.append(info)\n",
    "        \n",
    "        return pd.DataFrame(missing_info)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Mempelajari parameter imputation dari data training\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "        y : pandas Series atau numpy array, optional\n",
    "            Target variable (tidak digunakan)\n",
    "        \"\"\"\n",
    "        # Analisis missing values\n",
    "        self.missing_analysis = self.analyze_missing(X)\n",
    "\n",
    "        self._print_initial_summary(X)\n",
    "        \n",
    "        # Imputation berdasarkan strategi\n",
    "        if self.strategy == 'simple':\n",
    "            self._fit_simple(X)\n",
    "        else:\n",
    "            self._fit_advanced(X)\n",
    "            \n",
    "        self._print_imputation_summary()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_simple(self, X):\n",
    "        \"\"\"Implementasi simple imputation strategy\"\"\"\n",
    "        for col in X.columns:\n",
    "            if col in self.numerical_columns:\n",
    "                # Mean untuk kolom numerik\n",
    "                self.imputer_dict[col] = {\n",
    "                    'method': 'mean',\n",
    "                    'value': X[col].mean()\n",
    "                }\n",
    "            elif col in self.categorical_columns:\n",
    "                # mode untuk kolom kategorikal\n",
    "                mode_val = X[col].mode().iloc[0] if not X[col].mode().empty else \"MISSING\"\n",
    "                self.imputer_dict[col] = {\n",
    "                    'method': 'mode',\n",
    "                    'value': mode_val\n",
    "                }\n",
    "    \n",
    "    def _fit_advanced(self, X):\n",
    "        \"\"\"Implementasi advanced imputation strategy\"\"\"\n",
    "        for col in X.columns:\n",
    "            if col in self.numerical_columns:\n",
    "                non_null_data = X[col].dropna()\n",
    "                if abs(non_null_data.skew()) > 1:  # Data skewed\n",
    "                    self.imputer_dict[col] = {\n",
    "                        'method': 'median',\n",
    "                        'value': non_null_data.median()\n",
    "                    }\n",
    "                else:  # Distribusi normal\n",
    "                    self.imputer_dict[col] = {\n",
    "                        'method': 'mean',\n",
    "                        'value': non_null_data.mean()\n",
    "                    }\n",
    "                    \n",
    "            elif col in self.categorical_columns:\n",
    "                cardinality_ratio = X[col].nunique() / len(X)\n",
    "                if cardinality_ratio < 0.05:  # Kardinalitas rendah\n",
    "                    self.imputer_dict[col] = {\n",
    "                        'method': 'mode',\n",
    "                        'value': X[col].mode().iloc[0] if not X[col].mode().empty else \"MISSING\"\n",
    "                    }\n",
    "                else:  # Kardinalitas tinggi\n",
    "                    self.imputer_dict[col] = {\n",
    "                        'method': 'constant',\n",
    "                        'value': \"MISSING\"\n",
    "                    }\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Mengaplikasikan imputation pada data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas DataFrame\n",
    "            Data dengan missing values yang sudah diisi\n",
    "        \"\"\"\n",
    "        if not self.imputer_dict:\n",
    "            raise ValueError(\"Transformer belum di-fit. Panggil fit() terlebih dahulu.\")\n",
    "            \n",
    "        X_imputed = X.copy()\n",
    "        \n",
    "        for col, imputer_info in self.imputer_dict.items():\n",
    "            if col in X.columns:\n",
    "                X_imputed[col].fillna(imputer_info['value'], inplace=True)\n",
    "        \n",
    "        # Verifikasi hasil\n",
    "        remaining_missing = X_imputed.isnull().sum()\n",
    "        if remaining_missing.sum() > 0:\n",
    "            print(\"\\nWarning: Masih terdapat missing values:\")\n",
    "            print(remaining_missing[remaining_missing > 0])\n",
    "            \n",
    "        return X_imputed\n",
    "    \n",
    "    def _print_initial_summary(self, X):\n",
    "        \"\"\"Mencetak ringkasan awal missing values\"\"\"\n",
    "        print(\"\\nMissing Value Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_missing = X.isnull().sum().sum()\n",
    "        total_cells = X.size\n",
    "        \n",
    "        print(f\"Total missing values: {total_missing:,}\")\n",
    "        print(f\"Total cells: {total_cells:,}\")\n",
    "        print(f\"Overall missing percentage: {(total_missing/total_cells)*100:.2f}%\")\n",
    "        \n",
    "        if not self.missing_analysis.empty:\n",
    "            print(\"\\nKolom dengan missing values:\")\n",
    "            for _, row in self.missing_analysis.iterrows():\n",
    "                print(f\"\\n- {row['column']}\")\n",
    "                print(f\"  Missing: {row['missing_percentage']:.2f}%\")\n",
    "                if 'mean' in row:\n",
    "                    print(f\"  Mean: {row['mean']:.2f}\")\n",
    "                    print(f\"  Median: {row['median']:.2f}\")\n",
    "                    print(f\"  Skewness: {row['skewness']:.2f}\")\n",
    "                if 'unique_values' in row:\n",
    "                    print(f\"  Unique values: {row['unique_values']}\")\n",
    "                    print(f\"  Cardinality ratio: {row['cardinality_ratio']:.4f}\")\n",
    "    \n",
    "    def _print_imputation_summary(self):\n",
    "        \"\"\"Mencetak ringkasan metode imputation yang digunakan\"\"\"\n",
    "        print(\"\\nImputation Methods:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for col, info in self.imputer_dict.items():\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Method: {info['method']}\")\n",
    "            if isinstance(info['value'], (int, float)):\n",
    "                print(f\"  Value: {info['value']:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Value: {info['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_columns, method='zscore', threshold=3, strategy='clip'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        numerical_columns : list\n",
    "            List nama kolom numerik yang akan dihandle outliernya\n",
    "        method : str, default='zscore'\n",
    "            Metode deteksi outlier ('zscore', 'iqr')\n",
    "        threshold : float, default=3\n",
    "            Threshold untuk menentukan outlier:\n",
    "            - Untuk zscore: jumlah standar deviasi dari mean\n",
    "            - Untuk IQR: multiplier untuk IQR (biasanya 1.5)\n",
    "        strategy : str, default='clip'\n",
    "            Strategi handling outlier:\n",
    "            - 'clip': clip nilai ke batas atas/bawah\n",
    "            - 'remove': hapus baris dengan outlier (hanya untuk fit_transform)\n",
    "        \"\"\"\n",
    "        self.numerical_columns = numerical_columns\n",
    "        self.method = method\n",
    "        self.threshold = threshold\n",
    "        self.strategy = strategy\n",
    "        self.bounds = {}\n",
    "        \n",
    "    def detect_outliers(self, X, column):\n",
    "        \"\"\"\n",
    "        Mendeteksi outlier dalam satu kolom\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array\n",
    "            Boolean mask dimana True menandakan outlier\n",
    "        \"\"\"\n",
    "        if self.method == 'zscore':\n",
    "            z_scores = np.abs(stats.zscore(X[column].dropna()))\n",
    "            return z_scores > self.threshold\n",
    "            \n",
    "        elif self.method == 'iqr':\n",
    "            Q1 = X[column].quantile(0.25)\n",
    "            Q3 = X[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - (self.threshold * IQR)\n",
    "            upper_bound = Q3 + (self.threshold * IQR)\n",
    "            return (X[column] < lower_bound) | (X[column] > upper_bound)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Menghitung batas untuk setiap kolom numerik\n",
    "        \"\"\"\n",
    "        self.outlier_stats = {}\n",
    "        \n",
    "        print(\"\\nOutlier Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for column in self.numerical_columns:\n",
    "            if column in X.columns:\n",
    "                # Hitung statistik dasar\n",
    "                stats_dict = {\n",
    "                    'mean': X[column].mean(),\n",
    "                    'std': X[column].std(),\n",
    "                    'min': X[column].min(),\n",
    "                    'max': X[column].max(),\n",
    "                    'Q1': X[column].quantile(0.25),\n",
    "                    'Q3': X[column].quantile(0.75)\n",
    "                }\n",
    "                stats_dict['IQR'] = stats_dict['Q3'] - stats_dict['Q1']\n",
    "                \n",
    "                # Deteksi outlier\n",
    "                outlier_mask = self.detect_outliers(X, column)\n",
    "                non_null_data = X[column].dropna()\n",
    "                n_outliers = outlier_mask.sum()\n",
    "                \n",
    "                stats_dict.update({\n",
    "                    'n_outliers': n_outliers,\n",
    "                    'outlier_percentage': (n_outliers / len(non_null_data)) * 100\n",
    "                })\n",
    "                \n",
    "                # Tentukan bounds\n",
    "                if self.method == 'zscore':\n",
    "                    self.bounds[column] = {\n",
    "                        'lower': stats_dict['mean'] - (self.threshold * stats_dict['std']),\n",
    "                        'upper': stats_dict['mean'] + (self.threshold * stats_dict['std'])\n",
    "                    }\n",
    "                else:  # IQR method\n",
    "                    self.bounds[column] = {\n",
    "                        'lower': stats_dict['Q1'] - (self.threshold * stats_dict['IQR']),\n",
    "                        'upper': stats_dict['Q3'] + (self.threshold * stats_dict['IQR'])\n",
    "                    }\n",
    "                \n",
    "                stats_dict.update({\n",
    "                    'lower_bound': self.bounds[column]['lower'],\n",
    "                    'upper_bound': self.bounds[column]['upper']\n",
    "                })\n",
    "                \n",
    "                self.outlier_stats[column] = stats_dict\n",
    "                \n",
    "                self._print_column_summary(column, stats_dict)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Menangani outlier sesuai strategy yang dipilih\n",
    "        \"\"\"\n",
    "        if not self.bounds:\n",
    "            raise ValueError(\"Transformer belum di-fit. Panggil fit() terlebih dahulu.\")\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for column, bounds in self.bounds.items():\n",
    "            if column in X.columns:\n",
    "                if self.strategy == 'clip':\n",
    "                    X_transformed[column] = X_transformed[column].clip(\n",
    "                        lower=bounds['lower'],\n",
    "                        upper=bounds['upper']\n",
    "                    )\n",
    "                elif self.strategy == 'remove' and hasattr(self, 'outlier_stats'):\n",
    "                    # Mask untuk baris yang akan dipertahankan\n",
    "                    keep_mask = ~(\n",
    "                        (X_transformed[column] < bounds['lower']) |\n",
    "                        (X_transformed[column] > bounds['upper'])\n",
    "                    )\n",
    "                    X_transformed = X_transformed[keep_mask]\n",
    "        \n",
    "        if self.strategy == 'remove':\n",
    "            print(f\"\\nShape after removing outliers: {X_transformed.shape}\")\n",
    "            \n",
    "        return X_transformed\n",
    "    \n",
    "    def _print_column_summary(self, column, stats):\n",
    "        \"\"\"Mencetak ringkasan statistik dan outlier untuk satu kolom\"\"\"\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        print(f\"Original range: [{stats['min']:.2f}, {stats['max']:.2f}]\")\n",
    "        print(f\"Mean: {stats['mean']:.2f}\")\n",
    "        print(f\"Std: {stats['std']:.2f}\")\n",
    "        print(f\"Q1: {stats['Q1']:.2f}\")\n",
    "        print(f\"Q3: {stats['Q3']:.2f}\")\n",
    "        print(f\"IQR: {stats['IQR']:.2f}\")\n",
    "        print(f\"Outlier bounds: [{stats['lower_bound']:.2f}, {stats['upper_bound']:.2f}]\")\n",
    "        print(f\"Number of outliers: {stats['n_outliers']}\")\n",
    "        print(f\"Outlier percentage: {stats['outlier_percentage']:.2f}%\")\n",
    "    \n",
    "    def get_outlier_summary(self):\n",
    "        \"\"\"Mengembalikan ringkasan outlier dalam bentuk DataFrame\"\"\"\n",
    "        if not hasattr(self, 'outlier_stats'):\n",
    "            raise ValueError(\"No outlier statistics available. Run fit() first.\")\n",
    "        \n",
    "        summary_data = []\n",
    "        for column, stats in self.outlier_stats.items():\n",
    "            summary_data.append({\n",
    "                'column': column,\n",
    "                'outlier_count': stats['n_outliers'],\n",
    "                'outlier_percentage': stats['outlier_percentage'],\n",
    "                'lower_bound': stats['lower_bound'],\n",
    "                'upper_bound': stats['upper_bound']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, subset=None, keep='first', verbose=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        subset : list atau None, default=None\n",
    "            Kolom yang digunakan untuk identifikasi duplikat.\n",
    "            None berarti menggunakan semua kolom.\n",
    "        keep : {'first', 'last', False}, default='first'\n",
    "            - 'first' : Pertahankan kemunculan pertama dari duplikat\n",
    "            - 'last' : Pertahankan kemunculan terakhir dari duplikat\n",
    "            - False : Hapus semua duplikat\n",
    "        verbose : bool, default=True\n",
    "            Jika True, print informasi tentang duplikat yang ditemukan\n",
    "        \"\"\"\n",
    "        self.subset = subset\n",
    "        self.keep = keep\n",
    "        self.verbose = verbose\n",
    "        self.duplicate_info = None\n",
    "        \n",
    "    def analyze_duplicates(self, X):\n",
    "        \"\"\"\n",
    "        Menganalisis duplikat dalam dataset\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data yang akan dianalisis\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary berisi informasi tentang duplikat\n",
    "        \"\"\"\n",
    "        # Identifikasi duplikat\n",
    "        duplicates = X.duplicated(subset=self.subset, keep=False)\n",
    "        \n",
    "        # Hitung statistik duplikat\n",
    "        n_duplicates = duplicates.sum()\n",
    "        duplicate_rows = X[duplicates].index.tolist()\n",
    "        \n",
    "        duplicate_combinations = None\n",
    "        if self.subset is not None and n_duplicates > 0:\n",
    "            duplicate_combinations = (X[duplicates][self.subset]\n",
    "                                   .value_counts()\n",
    "                                   .reset_index()\n",
    "                                   .rename(columns={0: 'count'})\n",
    "                                   .query('count > 1'))\n",
    "        \n",
    "        return {\n",
    "            'total_rows': len(X),\n",
    "            'n_duplicates': n_duplicates,\n",
    "            'duplicate_percentage': (n_duplicates / len(X)) * 100,\n",
    "            'duplicate_rows': duplicate_rows,\n",
    "            'duplicate_combinations': duplicate_combinations\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Menganalisis duplikat dalam data training\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "        y : pandas Series atau numpy array, optional\n",
    "            Target variable (tidak digunakan)\n",
    "        \"\"\"\n",
    "        # Analisis duplikat\n",
    "        self.duplicate_info = self.analyze_duplicates(X)\n",
    "        \n",
    "        if self.verbose:\n",
    "            self._print_duplicate_summary()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Menghapus duplikat dari data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Data input\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas DataFrame\n",
    "            Data tanpa duplikat\n",
    "        \"\"\"\n",
    "\n",
    "        initial_shape = X.shape\n",
    "        \n",
    "        # Hapus duplikat\n",
    "        X_no_duplicates = X.drop_duplicates(subset=self.subset, keep=self.keep)\n",
    "        \n",
    "        if self.verbose:\n",
    "            rows_removed = initial_shape[0] - X_no_duplicates.shape[0]\n",
    "            if rows_removed > 0:\n",
    "                print(f\"\\nRemoved {rows_removed:,} duplicate rows\")\n",
    "                print(f\"Final shape: {X_no_duplicates.shape}\")\n",
    "        \n",
    "        return X_no_duplicates\n",
    "    \n",
    "    def _print_duplicate_summary(self):\n",
    "        \"\"\"Mencetak ringkasan analisis duplikat\"\"\"\n",
    "        if self.duplicate_info is None:\n",
    "            raise ValueError(\"No duplicate information available. Run fit() first.\")\n",
    "            \n",
    "        print(\"\\nDuplicate Analysis Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total rows: {self.duplicate_info['total_rows']:,}\")\n",
    "        print(f\"Duplicate rows: {self.duplicate_info['n_duplicates']:,}\")\n",
    "        print(f\"Duplicate percentage: {self.duplicate_info['duplicate_percentage']:.2f}%\")\n",
    "        \n",
    "        if (self.subset is not None and \n",
    "            self.duplicate_info['duplicate_combinations'] is not None and \n",
    "            not self.duplicate_info['duplicate_combinations'].empty):\n",
    "            \n",
    "            print(\"\\nDuplicate value combinations:\")\n",
    "            print(self.duplicate_info['duplicate_combinations'].to_string(index=False))\n",
    "    \n",
    "    def get_duplicate_summary(self):\n",
    "        \"\"\"\n",
    "        Mengembalikan ringkasan duplikat dalam bentuk dictionary\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary berisi informasi tentang duplikat\n",
    "        \"\"\"\n",
    "        if self.duplicate_info is None:\n",
    "            raise ValueError(\"No duplicate information available. Run fit() first.\")\n",
    "        \n",
    "        return self.duplicate_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Feature Engineering\n",
    "\n",
    "**Feature engineering** involves creating new features (input variables) or transforming existing ones to improve the performance of machine learning models. Feature engineering aims to enhance the model's ability to learn patterns and make accurate predictions from the data. It's often said that \"good features make good models.\"\n",
    "\n",
    "1. **Feature Selection:** Feature engineering can involve selecting the most relevant and informative features from the dataset. Removing irrelevant or redundant features not only simplifies the model but also reduces the risk of overfitting.\n",
    "\n",
    "2. **Creating New Features:** Sometimes, the existing features may not capture the underlying patterns effectively. In such cases, engineers create new features that provide additional information. For example:\n",
    "   \n",
    "   - **Polynomial Features:** Engineers may create new features by taking the square, cube, or other higher-order terms of existing numerical features. This can help capture nonlinear relationships.\n",
    "   \n",
    "   - **Interaction Features:** Interaction features are created by combining two or more existing features. For example, if you have features \"length\" and \"width,\" you can create an \"area\" feature by multiplying them.\n",
    "\n",
    "3. **Binning or Discretization:** Continuous numerical features can be divided into bins or categories. For instance, age values can be grouped into bins like \"child,\" \"adult,\" and \"senior.\"\n",
    "\n",
    "4. **Domain-Specific Feature Engineering:** Depending on the domain and problem, engineers may create domain-specific features. For example, in fraud detection, features related to transaction history and user behavior may be engineered to identify anomalies.\n",
    "\n",
    "Feature engineering is both a creative and iterative process. It requires a deep understanding of the data, domain knowledge, and experimentation to determine which features will enhance the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLWebsiteFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # 1. URL Complexity Score\n",
    "        X_new['URLComplexityScore'] = (\n",
    "            X_new['URLLength'] * 0.3 +\n",
    "            X_new['NoOfOtherSpecialCharsInURL'] * 0.3 +\n",
    "            X_new['NoOfQMarkInURL'] * 0.2 +\n",
    "            X_new['NoOfEqualsInURL'] * 0.2\n",
    "        )\n",
    "        \n",
    "        # 2. Character Distribution Features\n",
    "        X_new['SpecialCharToLengthRatio'] = (\n",
    "            (X_new['NoOfEqualsInURL'] + X_new['NoOfQMarkInURL'] + \n",
    "             X_new['NoOfOtherSpecialCharsInURL']) / X_new['URLLength']\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # 3. Domain Risk Score\n",
    "        X_new['DomainRiskScore'] = (\n",
    "            (X_new['DomainLength'] / X_new['URLLength']) * 0.4 +\n",
    "            (1 - X_new['DomainTitleMatchScore']) * 0.3 +\n",
    "            (1 - X_new['URLTitleMatchScore']) * 0.3\n",
    "        )\n",
    "        \n",
    "        # 4. Security Features Composite\n",
    "        security_features = ['IsHTTPS', 'HasFavicon', 'Robots', 'HasCopyrightInfo']\n",
    "        X_new['SecurityScore'] = X_new[security_features].sum(axis=1) / len(security_features)\n",
    "        \n",
    "        # 5. Content Richness Score\n",
    "        X_new['ContentRichnessScore'] = (\n",
    "            X_new['NoOfImage'] * 0.2 +\n",
    "            X_new['NoOfJS'] * 0.2 +\n",
    "            X_new['LineOfCode'] * 0.2 +\n",
    "            X_new['NoOfSelfRef'] * 0.2 +\n",
    "            X_new['NoOfExternalRef'] * 0.2\n",
    "        ) / X_new[['NoOfImage', 'NoOfJS', 'LineOfCode', 'NoOfSelfRef', 'NoOfExternalRef']].max().max()\n",
    "        \n",
    "        # 6. Interaction Elements Score\n",
    "        interaction_features = ['HasSubmitButton', 'HasHiddenFields', 'Pay', 'IsResponsive']\n",
    "        X_new['InteractionScore'] = X_new[interaction_features].sum(axis=1) / len(interaction_features)\n",
    "        \n",
    "        # 7. SEO Elements Score\n",
    "        seo_features = ['HasTitle', 'HasDescription', 'HasSocialNet']\n",
    "        X_new['SEOScore'] = X_new[seo_features].sum(axis=1) / len(seo_features)\n",
    "        \n",
    "        # 8. Numeric to Character Ratio\n",
    "        X_new['NumericToCharRatio'] = (\n",
    "            X_new['NoOfDegitsInURL'] / X_new['NoOfLettersInURL']\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # 9. URL Entropy Score\n",
    "        X_new['URLEntropyScore'] = (\n",
    "            X_new['CharContinuationRate'] * 0.5 +\n",
    "            X_new['URLCharProb'] * 0.5\n",
    "        )\n",
    "        \n",
    "        # New features\n",
    "        new_features = [\n",
    "            'URLComplexityScore', 'SpecialCharToLengthRatio', 'DomainRiskScore',\n",
    "            'SecurityScore', 'ContentRichnessScore', 'InteractionScore',\n",
    "            'SEOScore', 'NumericToCharRatio', 'URLEntropyScore'\n",
    "        ]\n",
    "            \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_infinite_values(df, columns):\n",
    "    \"\"\"\n",
    "    Handle infinite values in specified columns by replacing them with NaN\n",
    "    and then filling with column mean\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        mask = np.isinf(df[col])\n",
    "        if mask.any():\n",
    "            df.loc[mask, col] = np.nan\n",
    "\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprosesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler:\n",
    "    def __init__(self, method='standard'):\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "        self.numeric_columns = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Store numeric columns\n",
    "        self.numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.method == 'minmax':\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            self.scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'standard' or 'minmax'\")\n",
    "        \n",
    "        # Fit scaler untuk kolom numerical\n",
    "        if len(self.numeric_columns) > 0:\n",
    "            self.scaler.fit(X[self.numeric_columns])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        \n",
    "        # Transform untuk kolom numerical\n",
    "        if len(self.numeric_columns) > 0:\n",
    "            # Memastikan tidak ada infinite value\n",
    "            X_temp = X[self.numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "            X_temp = X_temp.fillna(X_temp.mean())\n",
    "            \n",
    "            X_scaled[self.numeric_columns] = self.scaler.transform(X_temp)\n",
    "        \n",
    "        return X_scaled\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A sklearn-compatible transformer for feature encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='label'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str, default='label'\n",
    "            Encoding method to use ('label' or 'onehot')\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.encoders = {}\n",
    "        self.categorical_columns = None\n",
    "        self.feature_names_out_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        if self.method == 'label':\n",
    "            for col in self.categorical_columns:\n",
    "                self.encoders[col] = LabelEncoder()\n",
    "                self.encoders[col].fit(X[col].astype(str))\n",
    "        else:  # onehot\n",
    "            for col in self.categorical_columns:\n",
    "                self.encoders[col] = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                self.encoders[col].fit(X[[col]])\n",
    "        \n",
    "        if self.method == 'onehot':\n",
    "            self.feature_names_out_ = []\n",
    "            for col in X.columns:\n",
    "                if col in self.categorical_columns:\n",
    "                    self.feature_names_out_.extend(\n",
    "                        [f\"{col}_{cat}\" for cat in self.encoders[col].get_feature_names_out([col])]\n",
    "                    )\n",
    "                else:\n",
    "                    self.feature_names_out_.append(col)\n",
    "        else:\n",
    "            self.feature_names_out_ = list(X.columns)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        \n",
    "        if self.method == 'label':\n",
    "            # Transform menggunakan label encoder\n",
    "            for col in self.categorical_columns:\n",
    "                X_encoded[col] = self.encoders[col].transform(X[col].astype(str))\n",
    "        else:  # onehot\n",
    "            # Transform menggunakan one-hot encoder\n",
    "            for col in self.categorical_columns:\n",
    "                # Get encoded column names\n",
    "                encoded_features = [f\"{col}_{cat}\" for cat in \n",
    "                                 self.encoders[col].get_feature_names_out([col])]\n",
    "                \n",
    "                # Transform dan add new columns\n",
    "                encoded_cols = self.encoders[col].transform(X[[col]])\n",
    "                encoded_df = pd.DataFrame(encoded_cols, columns=encoded_features, \n",
    "                                      index=X.index)\n",
    "                \n",
    "                # Drop original column dan add hasil encoded \n",
    "                X_encoded = X_encoded.drop(col, axis=1)\n",
    "                X_encoded = pd.concat([X_encoded, encoded_df], axis=1)\n",
    "        \n",
    "        return X_encoded\n",
    "    \n",
    "    def get_feature_names_out(self, feature_names_in=None):\n",
    "        \"\"\"Return feature names for output features.\"\"\"\n",
    "        return self.feature_names_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalanceHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A sklearn-compatible transformer for handling imbalanced datasets.\n",
    "    Note: This transformer modifies both X and y, so it should typically\n",
    "    be used in a separate pipeline for preprocessing.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='smote', sampling_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str, default='smote'\n",
    "            Resampling method to use ('smote', 'oversample', or 'undersample')\n",
    "        sampling_ratio : float, default=1.0\n",
    "            Ratio of samples in minority class to majority class\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        \n",
    "        # Initialize resampler berdasarkan method\n",
    "        if method == 'smote':\n",
    "            self.resampler = SMOTE(sampling_strategy=sampling_ratio, random_state=42)\n",
    "        elif method == 'oversample':\n",
    "            self.resampler = RandomOverSampler(sampling_strategy=sampling_ratio, random_state=42)\n",
    "        elif method == 'undersample':\n",
    "            self.resampler = RandomUnderSampler(sampling_strategy=sampling_ratio, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'smote', 'oversample', or 'undersample'\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.feature_names_ = X.columns if isinstance(X, pd.DataFrame) else None\n",
    "        \n",
    "        print(\"Class distribution before resampling:\")\n",
    "        print(Counter(y))\n",
    "        \n",
    "        self.resampler.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        if y is None:\n",
    "            return X\n",
    "            \n",
    "        # Perform resampling\n",
    "        X_resampled, y_resampled = self.resampler.fit_resample(X, y)\n",
    "        \n",
    "        print(\"\\nClass distribution after resampling:\")\n",
    "        print(Counter(y_resampled))\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_resampled = pd.DataFrame(X_resampled, columns=self.feature_names_)\n",
    "        \n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline untuk data cleaning\n",
    "cleaning_pipe = Pipeline([\n",
    "    (\"feature_selector\", FeatureSelector(\n",
    "        categorical_columns=categorical_columns,\n",
    "        numerical_columns=None, \n",
    "        missing_threshold=70,\n",
    "        correlation_threshold=0.1\n",
    "    )),\n",
    "    (\"missing_handler\", MissingValueHandler(\n",
    "        categorical_columns=categorical_columns,\n",
    "        numerical_columns=None, \n",
    "        strategy='advanced'\n",
    "    )),\n",
    "    (\"outlier_handler\", OutlierHandler(\n",
    "        numerical_columns=None, \n",
    "        method='zscore',\n",
    "        threshold=3,\n",
    "        strategy='clip'\n",
    "    )),\n",
    "    (\"duplicate_handler\", DuplicateHandler(\n",
    "        subset=None, \n",
    "        keep='first'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Pipeline untuk preprocessing fitur\n",
    "feature_pipe = Pipeline([\n",
    "    (\"scaler\", FeatureScaler(method='standard')),\n",
    "    (\"encoder\", FeatureEncoder(method='label'))\n",
    "])\n",
    "\n",
    "# Pipeline untuk handling imbalanced data\n",
    "imbalance_pipe = Pipeline([\n",
    "    (\"imbalance\", ImbalanceHandler(method='smote', sampling_ratio=1.0))\n",
    "])\n",
    "\n",
    "def apply_complete_preprocessing(train_set, val_set, y_train=None):\n",
    "    \"\"\"\n",
    "    Menerapkan complete preprocessing pipeline pada data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_set : pandas.DataFrame\n",
    "        Dataset training\n",
    "    val_set : pandas.DataFrame\n",
    "        Dataset validasi\n",
    "    y_train : pandas.Series, optional\n",
    "        Label training untuk imbalance handling\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (processed_train, processed_val, y_resampled)\n",
    "        y_resampled hanya direturn jika y_train disediakan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Step 1: Applying data cleaning pipeline...\")\n",
    "        cleaned_train = cleaning_pipe.fit_transform(train_set)\n",
    "        cleaned_val = cleaning_pipe.transform(val_set)\n",
    "        \n",
    "        print(\"\\nStep 2: Applying feature preprocessing pipeline...\")\n",
    "        processed_train = feature_pipe.fit_transform(cleaned_train)\n",
    "        processed_val = feature_pipe.transform(cleaned_val)\n",
    "        \n",
    "        # Jika y_train disediakan, lakukan imbalance handling\n",
    "        if y_train is not None:\n",
    "            print(\"\\nStep 3: Applying imbalance handling...\")\n",
    "            processed_train, y_resampled = imbalance_pipe.fit_transform(processed_train, y_train)\n",
    "            print(\"\\nAll preprocessing steps completed successfully!\")\n",
    "            return processed_train, processed_val, y_resampled\n",
    "        \n",
    "        print(\"\\nPreprocessing steps completed successfully!\")\n",
    "        return processed_train, processed_val\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during preprocessing: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNFromScratch:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3, metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int, default=3\n",
    "            Number of neighbors\n",
    "        metric : str, default='euclidean'\n",
    "            Distance metric to use ('euclidean', 'manhattan', or 'minkowski')\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.metric = metric.lower()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using X as training data and y as target values.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        self.X_train = np.asarray(X)\n",
    "        self.y_train = np.asarray(y)\n",
    "        return self\n",
    "    \n",
    "    def _calculate_distance(self, x1, x2):\n",
    "        \"\"\"Calculate distance between two points using specified metric.\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        elif self.metric == 'minkowski':\n",
    "            # Using p=3 for Minkowski distance\n",
    "            return np.power(np.sum(np.abs(x1 - x2) ** 3), 1/3)\n",
    "        else:\n",
    "            raise ValueError(\"Metric must be 'euclidean', 'manhattan', or 'minkowski'\")\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        distances = [self._calculate_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        \n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y_pred = np.array([self._predict_single(x) for x in X])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayesFromScratch:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implementation from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.vars = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Gaussian Naive Bayes according to X, y.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.vars = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.means[idx, :] = X_c.mean(axis=0)\n",
    "            self.vars[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / n_samples\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        \"\"\"Calculate Gaussian likelihood P(x|class).\"\"\"\n",
    "        eps = 1e-10\n",
    "        exponent = -0.5 * np.log(2 * np.pi * (var + eps)) - ((x - mean) ** 2) / (2 * (var + eps))\n",
    "        return np.sum(exponent)\n",
    "    \n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, _ in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            likelihood = self._calculate_likelihood(x, self.means[idx, :], self.vars[idx, :])\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y_pred = np.array([self._predict_single(x) for x in X])\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array of shape (n_samples, n_classes)\n",
    "            Probability estimates for each class\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        probas = []\n",
    "        \n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for idx, _ in enumerate(self.classes):\n",
    "                prior = np.log(self.priors[idx])\n",
    "                likelihood = self._calculate_likelihood(x, self.means[idx, :], self.vars[idx, :])\n",
    "                posteriors.append(prior + likelihood)\n",
    "            \n",
    "            posteriors = np.array(posteriors)\n",
    "            posteriors = np.exp(posteriors - np.max(posteriors))\n",
    "            posteriors = posteriors / np.sum(posteriors)\n",
    "            probas.append(posteriors)\n",
    "            \n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train` folder given by the TA. The `test` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, val_set = train_test_split(df_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Ukuran dataset original: 140404 sampel\n",
      "Ukuran dataset sampel: 42121 sampel\n",
      "Distribusi kelas dalam sampel:\n",
      "label\n",
      "1    0.924\n",
      "0    0.076\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Ukuran data training: 33696\n",
      "Ukuran data validasi: 8425\n",
      "\n",
      "Jumlah kolom kategorikal: 22\n",
      "Jumlah kolom numerikal: 33\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Missing Value Analysis:\n",
      "--------------------------------------------------\n",
      "Total missing values: 715,469\n",
      "Total cells: 1,853,280\n",
      "Overall missing percentage: 38.61%\n",
      "\n",
      "Kolom dengan missing values:\n",
      "\n",
      "- FILENAME\n",
      "  Missing: 41.25%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 19798.0\n",
      "  Cardinality ratio: 0.5875\n",
      "\n",
      "- URL\n",
      "  Missing: 31.39%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 23120.0\n",
      "  Cardinality ratio: 0.6861\n",
      "\n",
      "- URLLength\n",
      "  Missing: 43.14%\n",
      "  Mean: 27.92\n",
      "  Median: 26.00\n",
      "  Skewness: 101.30\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- Domain\n",
      "  Missing: 50.34%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 16672.0\n",
      "  Cardinality ratio: 0.4948\n",
      "\n",
      "- DomainLength\n",
      "  Missing: 33.12%\n",
      "  Mean: 19.66\n",
      "  Median: 19.00\n",
      "  Skewness: 2.16\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- IsDomainIP\n",
      "  Missing: 30.13%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- TLD\n",
      "  Missing: 32.15%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 316.0\n",
      "  Cardinality ratio: 0.0094\n",
      "\n",
      "- CharContinuationRate\n",
      "  Missing: 34.40%\n",
      "  Mean: 0.92\n",
      "  Median: 1.00\n",
      "  Skewness: -1.90\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- TLDLegitimateProb\n",
      "  Missing: 37.30%\n",
      "  Mean: 0.28\n",
      "  Median: 0.52\n",
      "  Skewness: -0.03\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- URLCharProb\n",
      "  Missing: 36.93%\n",
      "  Mean: 0.06\n",
      "  Median: 0.06\n",
      "  Skewness: -1.18\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- TLDLength\n",
      "  Missing: 33.82%\n",
      "  Mean: 2.73\n",
      "  Median: 3.00\n",
      "  Skewness: 1.32\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfSubDomain\n",
      "  Missing: 31.23%\n",
      "  Mean: 1.17\n",
      "  Median: 1.00\n",
      "  Skewness: 2.38\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- HasObfuscation\n",
      "  Missing: 46.96%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- NoOfObfuscatedChar\n",
      "  Missing: 47.55%\n",
      "  Mean: 0.02\n",
      "  Median: 0.00\n",
      "  Skewness: 131.56\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- ObfuscationRatio\n",
      "  Missing: 45.75%\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "  Skewness: 88.62\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfLettersInURL\n",
      "  Missing: 45.14%\n",
      "  Mean: 14.20\n",
      "  Median: 13.00\n",
      "  Skewness: 54.35\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- LetterRatioInURL\n",
      "  Missing: 46.78%\n",
      "  Mean: 0.48\n",
      "  Median: 0.48\n",
      "  Skewness: -0.19\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfDegitsInURL\n",
      "  Missing: 42.01%\n",
      "  Mean: 0.34\n",
      "  Median: 0.00\n",
      "  Skewness: 19.93\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- DegitRatioInURL\n",
      "  Missing: 37.82%\n",
      "  Mean: 0.01\n",
      "  Median: 0.00\n",
      "  Skewness: 6.90\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfEqualsInURL\n",
      "  Missing: 43.65%\n",
      "  Mean: 0.01\n",
      "  Median: 0.00\n",
      "  Skewness: 27.74\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfQMarkInURL\n",
      "  Missing: 31.72%\n",
      "  Mean: 0.01\n",
      "  Median: 0.00\n",
      "  Skewness: 19.37\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfAmpersandInURL\n",
      "  Missing: 32.34%\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "  Skewness: 97.28\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfOtherSpecialCharsInURL\n",
      "  Missing: 34.08%\n",
      "  Mean: 1.45\n",
      "  Median: 1.00\n",
      "  Skewness: 119.38\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- SpacialCharRatioInURL\n",
      "  Missing: 45.09%\n",
      "  Mean: 0.05\n",
      "  Median: 0.04\n",
      "  Skewness: 1.73\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- IsHTTPS\n",
      "  Missing: 35.32%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- LineOfCode\n",
      "  Missing: 49.37%\n",
      "  Mean: 1784.57\n",
      "  Median: 1006.00\n",
      "  Skewness: 11.54\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- LargestLineLength\n",
      "  Missing: 48.57%\n",
      "  Mean: 8049.58\n",
      "  Median: 2710.00\n",
      "  Skewness: 47.79\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- HasTitle\n",
      "  Missing: 31.69%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- Title\n",
      "  Missing: 41.14%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 19278.0\n",
      "  Cardinality ratio: 0.5721\n",
      "\n",
      "- DomainTitleMatchScore\n",
      "  Missing: 35.40%\n",
      "  Mean: 70.79\n",
      "  Median: 100.00\n",
      "  Skewness: -0.93\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- URLTitleMatchScore\n",
      "  Missing: 37.19%\n",
      "  Mean: 70.87\n",
      "  Median: 100.00\n",
      "  Skewness: -0.93\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- HasFavicon\n",
      "  Missing: 41.38%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- Robots\n",
      "  Missing: 32.99%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- IsResponsive\n",
      "  Missing: 30.34%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- NoOfURLRedirect\n",
      "  Missing: 47.91%\n",
      "  Mean: 0.12\n",
      "  Median: 0.00\n",
      "  Skewness: 2.29\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfSelfRedirect\n",
      "  Missing: 47.33%\n",
      "  Mean: 0.03\n",
      "  Median: 0.00\n",
      "  Skewness: 5.48\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- HasDescription\n",
      "  Missing: 39.02%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- NoOfPopup\n",
      "  Missing: 30.84%\n",
      "  Mean: 0.35\n",
      "  Median: 0.00\n",
      "  Skewness: 50.51\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfiFrame\n",
      "  Missing: 35.80%\n",
      "  Mean: 2.45\n",
      "  Median: 1.00\n",
      "  Skewness: 10.59\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- HasExternalFormSubmit\n",
      "  Missing: 39.38%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- HasSocialNet\n",
      "  Missing: 48.16%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- HasSubmitButton\n",
      "  Missing: 44.26%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- HasHiddenFields\n",
      "  Missing: 31.39%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- HasPasswordField\n",
      "  Missing: 47.32%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- Bank\n",
      "  Missing: 38.97%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- Pay\n",
      "  Missing: 30.56%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- Crypto\n",
      "  Missing: 35.56%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- HasCopyrightInfo\n",
      "  Missing: 47.77%\n",
      "  Mean: nan\n",
      "  Median: nan\n",
      "  Skewness: nan\n",
      "  Unique values: 2.0\n",
      "  Cardinality ratio: 0.0001\n",
      "\n",
      "- NoOfImage\n",
      "  Missing: 35.88%\n",
      "  Mean: 40.99\n",
      "  Median: 23.00\n",
      "  Skewness: 22.02\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfCSS\n",
      "  Missing: 48.12%\n",
      "  Mean: 9.79\n",
      "  Median: 6.00\n",
      "  Skewness: 8.02\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfJS\n",
      "  Missing: 43.21%\n",
      "  Mean: 16.56\n",
      "  Median: 12.00\n",
      "  Skewness: 20.80\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfSelfRef\n",
      "  Missing: 34.23%\n",
      "  Mean: 106.21\n",
      "  Median: 69.00\n",
      "  Skewness: 55.83\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfEmptyRef\n",
      "  Missing: 30.46%\n",
      "  Mean: 3.87\n",
      "  Median: 0.00\n",
      "  Skewness: 27.51\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "- NoOfExternalRef\n",
      "  Missing: 49.69%\n",
      "  Mean: 77.40\n",
      "  Median: 39.00\n",
      "  Skewness: 31.08\n",
      "  Unique values: nan\n",
      "  Cardinality ratio: nan\n",
      "\n",
      "Imputation Methods:\n",
      "--------------------------------------------------\n",
      "\n",
      "id:\n",
      "  Method: mean\n",
      "  Value: 117355.6023\n",
      "\n",
      "FILENAME:\n",
      "  Method: constant\n",
      "  Value: MISSING\n",
      "\n",
      "URL:\n",
      "  Method: constant\n",
      "  Value: MISSING\n",
      "\n",
      "URLLength:\n",
      "  Method: median\n",
      "  Value: 26.0000\n",
      "\n",
      "Domain:\n",
      "  Method: constant\n",
      "  Value: MISSING\n",
      "\n",
      "DomainLength:\n",
      "  Method: median\n",
      "  Value: 19.0000\n",
      "\n",
      "IsDomainIP:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "TLD:\n",
      "  Method: mode\n",
      "  Value: com\n",
      "\n",
      "CharContinuationRate:\n",
      "  Method: median\n",
      "  Value: 1.0000\n",
      "\n",
      "TLDLegitimateProb:\n",
      "  Method: mean\n",
      "  Value: 0.2763\n",
      "\n",
      "URLCharProb:\n",
      "  Method: median\n",
      "  Value: 0.0602\n",
      "\n",
      "TLDLength:\n",
      "  Method: median\n",
      "  Value: 3.0000\n",
      "\n",
      "NoOfSubDomain:\n",
      "  Method: median\n",
      "  Value: 1.0000\n",
      "\n",
      "HasObfuscation:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfObfuscatedChar:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "ObfuscationRatio:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfLettersInURL:\n",
      "  Method: median\n",
      "  Value: 13.0000\n",
      "\n",
      "LetterRatioInURL:\n",
      "  Method: mean\n",
      "  Value: 0.4830\n",
      "\n",
      "NoOfDegitsInURL:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "DegitRatioInURL:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfEqualsInURL:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfQMarkInURL:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfAmpersandInURL:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfOtherSpecialCharsInURL:\n",
      "  Method: median\n",
      "  Value: 1.0000\n",
      "\n",
      "SpacialCharRatioInURL:\n",
      "  Method: median\n",
      "  Value: 0.0430\n",
      "\n",
      "IsHTTPS:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "LineOfCode:\n",
      "  Method: median\n",
      "  Value: 1006.0000\n",
      "\n",
      "LargestLineLength:\n",
      "  Method: median\n",
      "  Value: 2710.0000\n",
      "\n",
      "HasTitle:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "Title:\n",
      "  Method: constant\n",
      "  Value: MISSING\n",
      "\n",
      "DomainTitleMatchScore:\n",
      "  Method: mean\n",
      "  Value: 70.7890\n",
      "\n",
      "URLTitleMatchScore:\n",
      "  Method: mean\n",
      "  Value: 70.8738\n",
      "\n",
      "HasFavicon:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "Robots:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "IsResponsive:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "NoOfURLRedirect:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfSelfRedirect:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "HasDescription:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "NoOfPopup:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfiFrame:\n",
      "  Method: median\n",
      "  Value: 1.0000\n",
      "\n",
      "HasExternalFormSubmit:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "HasSocialNet:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "HasSubmitButton:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "HasHiddenFields:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "HasPasswordField:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "Bank:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "Pay:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "Crypto:\n",
      "  Method: mode\n",
      "  Value: 0.0000\n",
      "\n",
      "HasCopyrightInfo:\n",
      "  Method: mode\n",
      "  Value: 1.0000\n",
      "\n",
      "NoOfImage:\n",
      "  Method: median\n",
      "  Value: 23.0000\n",
      "\n",
      "NoOfCSS:\n",
      "  Method: median\n",
      "  Value: 6.0000\n",
      "\n",
      "NoOfJS:\n",
      "  Method: median\n",
      "  Value: 12.0000\n",
      "\n",
      "NoOfSelfRef:\n",
      "  Method: median\n",
      "  Value: 69.0000\n",
      "\n",
      "NoOfEmptyRef:\n",
      "  Method: median\n",
      "  Value: 0.0000\n",
      "\n",
      "NoOfExternalRef:\n",
      "  Method: median\n",
      "  Value: 39.0000\n",
      "\n",
      "Handling outliers...\n",
      "\n",
      "Outlier Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Column: id\n",
      "Original range: [1.00, 235790.00]\n",
      "Mean: 117355.60\n",
      "Std: 68192.66\n",
      "Q1: 58318.75\n",
      "Q3: 176435.25\n",
      "IQR: 118116.50\n",
      "Outlier bounds: [-87222.38, 321933.58]\n",
      "Number of outliers: 0\n",
      "Outlier percentage: 0.00%\n",
      "\n",
      "Column: URLLength\n",
      "Original range: [15.00, 4054.00]\n",
      "Mean: 27.09\n",
      "Std: 24.49\n",
      "Q1: 25.00\n",
      "Q3: 27.00\n",
      "IQR: 2.00\n",
      "Outlier bounds: [-46.39, 100.57]\n",
      "Number of outliers: 82\n",
      "Outlier percentage: 0.24%\n",
      "\n",
      "Column: DomainLength\n",
      "Original range: [4.00, 87.00]\n",
      "Mean: 19.44\n",
      "Std: 4.87\n",
      "Q1: 17.00\n",
      "Q3: 21.00\n",
      "IQR: 4.00\n",
      "Outlier bounds: [4.84, 34.04]\n",
      "Number of outliers: 380\n",
      "Outlier percentage: 1.13%\n",
      "\n",
      "Column: CharContinuationRate\n",
      "Original range: [0.18, 1.00]\n",
      "Mean: 0.95\n",
      "Std: 0.14\n",
      "Q1: 1.00\n",
      "Q3: 1.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [0.54, 1.35]\n",
      "Number of outliers: 918\n",
      "Outlier percentage: 2.72%\n",
      "\n",
      "Column: TLDLegitimateProb\n",
      "Original range: [0.00, 0.52]\n",
      "Mean: 0.28\n",
      "Std: 0.20\n",
      "Q1: 0.08\n",
      "Q3: 0.52\n",
      "IQR: 0.44\n",
      "Outlier bounds: [-0.31, 0.87]\n",
      "Number of outliers: 0\n",
      "Outlier percentage: 0.00%\n",
      "\n",
      "Column: URLCharProb\n",
      "Original range: [0.00, 0.09]\n",
      "Mean: 0.06\n",
      "Std: 0.01\n",
      "Q1: 0.06\n",
      "Q3: 0.06\n",
      "IQR: 0.00\n",
      "Outlier bounds: [0.04, 0.08]\n",
      "Number of outliers: 651\n",
      "Outlier percentage: 1.93%\n",
      "\n",
      "Column: TLDLength\n",
      "Original range: [2.00, 13.00]\n",
      "Mean: 2.82\n",
      "Std: 0.46\n",
      "Q1: 3.00\n",
      "Q3: 3.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [1.45, 4.20]\n",
      "Number of outliers: 108\n",
      "Outlier percentage: 0.32%\n",
      "\n",
      "Column: NoOfSubDomain\n",
      "Original range: [0.00, 7.00]\n",
      "Mean: 1.11\n",
      "Std: 0.39\n",
      "Q1: 1.00\n",
      "Q3: 1.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.04, 2.27]\n",
      "Number of outliers: 375\n",
      "Outlier percentage: 1.11%\n",
      "\n",
      "Column: NoOfObfuscatedChar\n",
      "Original range: [0.00, 291.00]\n",
      "Mean: 0.01\n",
      "Std: 1.59\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-4.76, 4.78]\n",
      "Number of outliers: 3\n",
      "Outlier percentage: 0.01%\n",
      "\n",
      "Column: ObfuscationRatio\n",
      "Original range: [0.00, 0.21]\n",
      "Mean: 0.00\n",
      "Std: 0.00\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.00, 0.00]\n",
      "Number of outliers: 8\n",
      "Outlier percentage: 0.02%\n",
      "\n",
      "Column: NoOfLettersInURL\n",
      "Original range: [0.00, 1521.00]\n",
      "Mean: 13.66\n",
      "Std: 11.53\n",
      "Q1: 12.00\n",
      "Q3: 13.00\n",
      "IQR: 1.00\n",
      "Outlier bounds: [-20.92, 48.24]\n",
      "Number of outliers: 171\n",
      "Outlier percentage: 0.51%\n",
      "\n",
      "Column: LetterRatioInURL\n",
      "Original range: [0.06, 0.84]\n",
      "Mean: 0.48\n",
      "Std: 0.07\n",
      "Q1: 0.48\n",
      "Q3: 0.50\n",
      "IQR: 0.02\n",
      "Outlier bounds: [0.26, 0.71]\n",
      "Number of outliers: 542\n",
      "Outlier percentage: 1.61%\n",
      "\n",
      "Column: NoOfDegitsInURL\n",
      "Original range: [0.00, 110.00]\n",
      "Mean: 0.20\n",
      "Std: 2.04\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-5.91, 6.31]\n",
      "Number of outliers: 261\n",
      "Outlier percentage: 0.77%\n",
      "\n",
      "Column: DegitRatioInURL\n",
      "Original range: [0.00, 0.60]\n",
      "Mean: 0.00\n",
      "Std: 0.03\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.07, 0.08]\n",
      "Number of outliers: 690\n",
      "Outlier percentage: 2.05%\n",
      "\n",
      "Column: NoOfEqualsInURL\n",
      "Original range: [0.00, 9.00]\n",
      "Mean: 0.01\n",
      "Std: 0.16\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.48, 0.50]\n",
      "Number of outliers: 79\n",
      "Outlier percentage: 0.23%\n",
      "\n",
      "Column: NoOfQMarkInURL\n",
      "Original range: [0.00, 3.00]\n",
      "Mean: 0.00\n",
      "Std: 0.07\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.21, 0.22]\n",
      "Number of outliers: 116\n",
      "Outlier percentage: 0.34%\n",
      "\n",
      "Column: NoOfAmpersandInURL\n",
      "Original range: [0.00, 27.00]\n",
      "Mean: 0.00\n",
      "Std: 0.18\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.54, 0.55]\n",
      "Number of outliers: 12\n",
      "Outlier percentage: 0.04%\n",
      "\n",
      "Column: NoOfOtherSpecialCharsInURL\n",
      "Original range: [0.00, 470.00]\n",
      "Mean: 1.30\n",
      "Std: 2.76\n",
      "Q1: 1.00\n",
      "Q3: 1.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-6.99, 9.58]\n",
      "Number of outliers: 77\n",
      "Outlier percentage: 0.23%\n",
      "\n",
      "Column: SpacialCharRatioInURL\n",
      "Original range: [0.00, 0.26]\n",
      "Mean: 0.05\n",
      "Std: 0.02\n",
      "Q1: 0.04\n",
      "Q3: 0.04\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.00, 0.10]\n",
      "Number of outliers: 880\n",
      "Outlier percentage: 2.61%\n",
      "\n",
      "Column: LineOfCode\n",
      "Original range: [2.00, 113302.00]\n",
      "Mean: 1400.16\n",
      "Std: 2200.54\n",
      "Q1: 994.00\n",
      "Q3: 1021.00\n",
      "IQR: 27.00\n",
      "Outlier bounds: [-5201.46, 8001.78]\n",
      "Number of outliers: 491\n",
      "Outlier percentage: 1.46%\n",
      "\n",
      "Column: LargestLineLength\n",
      "Original range: [25.00, 2830363.00]\n",
      "Mean: 5456.17\n",
      "Std: 23215.06\n",
      "Q1: 2645.75\n",
      "Q3: 2986.00\n",
      "IQR: 340.25\n",
      "Outlier bounds: [-64189.02, 75101.36]\n",
      "Number of outliers: 128\n",
      "Outlier percentage: 0.38%\n",
      "\n",
      "Column: DomainTitleMatchScore\n",
      "Original range: [0.00, 100.00]\n",
      "Mean: 70.79\n",
      "Std: 36.21\n",
      "Q1: 70.79\n",
      "Q3: 100.00\n",
      "IQR: 29.21\n",
      "Outlier bounds: [-37.83, 179.41]\n",
      "Number of outliers: 0\n",
      "Outlier percentage: 0.00%\n",
      "\n",
      "Column: URLTitleMatchScore\n",
      "Original range: [0.00, 100.00]\n",
      "Mean: 70.87\n",
      "Std: 35.66\n",
      "Q1: 70.87\n",
      "Q3: 100.00\n",
      "IQR: 29.13\n",
      "Outlier bounds: [-36.10, 177.85]\n",
      "Number of outliers: 0\n",
      "Outlier percentage: 0.00%\n",
      "\n",
      "Column: NoOfURLRedirect\n",
      "Original range: [0.00, 1.00]\n",
      "Mean: 0.06\n",
      "Std: 0.25\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.67, 0.80]\n",
      "Number of outliers: 2169\n",
      "Outlier percentage: 6.44%\n",
      "\n",
      "Column: NoOfSelfRedirect\n",
      "Original range: [0.00, 1.00]\n",
      "Mean: 0.02\n",
      "Std: 0.13\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-0.36, 0.39]\n",
      "Number of outliers: 538\n",
      "Outlier percentage: 1.60%\n",
      "\n",
      "Column: NoOfPopup\n",
      "Original range: [0.00, 405.00]\n",
      "Mean: 0.25\n",
      "Std: 3.62\n",
      "Q1: 0.00\n",
      "Q3: 0.00\n",
      "IQR: 0.00\n",
      "Outlier bounds: [-10.60, 11.09]\n",
      "Number of outliers: 122\n",
      "Outlier percentage: 0.36%\n",
      "\n",
      "Column: NoOfiFrame\n",
      "Original range: [0.00, 278.00]\n",
      "Mean: 1.93\n",
      "Std: 4.75\n",
      "Q1: 0.00\n",
      "Q3: 1.00\n",
      "IQR: 1.00\n",
      "Outlier bounds: [-12.32, 16.18]\n",
      "Number of outliers: 1065\n",
      "Outlier percentage: 3.16%\n",
      "\n",
      "Column: NoOfImage\n",
      "Original range: [0.00, 5699.00]\n",
      "Mean: 34.54\n",
      "Std: 77.42\n",
      "Q1: 17.00\n",
      "Q3: 30.00\n",
      "IQR: 13.00\n",
      "Outlier bounds: [-197.73, 266.80]\n",
      "Number of outliers: 302\n",
      "Outlier percentage: 0.90%\n",
      "\n",
      "Column: NoOfCSS\n",
      "Original range: [0.00, 439.00]\n",
      "Mean: 7.97\n",
      "Std: 10.54\n",
      "Q1: 5.00\n",
      "Q3: 6.00\n",
      "IQR: 1.00\n",
      "Outlier bounds: [-23.67, 39.60]\n",
      "Number of outliers: 554\n",
      "Outlier percentage: 1.64%\n",
      "\n",
      "Column: NoOfJS\n",
      "Original range: [0.00, 1326.00]\n",
      "Mean: 14.59\n",
      "Std: 15.87\n",
      "Q1: 11.00\n",
      "Q3: 14.00\n",
      "IQR: 3.00\n",
      "Outlier bounds: [-33.01, 62.19]\n",
      "Number of outliers: 342\n",
      "Outlier percentage: 1.01%\n",
      "\n",
      "Column: NoOfSelfRef\n",
      "Original range: [0.00, 24455.00]\n",
      "Mean: 93.47\n",
      "Std: 217.72\n",
      "Q1: 49.00\n",
      "Q3: 94.00\n",
      "IQR: 45.00\n",
      "Outlier bounds: [-559.68, 746.63]\n",
      "Number of outliers: 145\n",
      "Outlier percentage: 0.43%\n",
      "\n",
      "Column: NoOfEmptyRef\n",
      "Original range: [0.00, 1336.00]\n",
      "Mean: 2.69\n",
      "Std: 18.66\n",
      "Q1: 0.00\n",
      "Q3: 1.00\n",
      "IQR: 1.00\n",
      "Outlier bounds: [-53.30, 58.68]\n",
      "Number of outliers: 170\n",
      "Outlier percentage: 0.50%\n",
      "\n",
      "Column: NoOfExternalRef\n",
      "Original range: [0.00, 11901.00]\n",
      "Mean: 58.32\n",
      "Std: 115.77\n",
      "Q1: 38.00\n",
      "Q3: 39.00\n",
      "IQR: 1.00\n",
      "Outlier bounds: [-288.99, 405.63]\n",
      "Number of outliers: 281\n",
      "Outlier percentage: 0.83%\n",
      "\n",
      "Applying feature engineering...\n",
      "\n",
      "Handling infinite values...\n",
      "\n",
      "Scaling features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[401], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScaling features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m scaler \u001b[38;5;241m=\u001b[39m FeatureScaler(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_featured\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_val_featured)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 5. Feature Encoding\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[394], line 48\u001b[0m, in \u001b[0;36mFeatureScaler.fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[1;32mIn[394], line 30\u001b[0m, in \u001b[0;36mFeatureScaler.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Fit scaler only on numeric columns\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumeric_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Mengambil 30% sampel dari data\n",
    "sample_size = int(len(df) * 0.3)\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Memisahkan fitur dan target dari sampel\n",
    "X = df_sample.drop('label', axis=1)\n",
    "y = df_sample['label']\n",
    "\n",
    "print(f\"Ukuran dataset original: {len(df)} sampel\")\n",
    "print(f\"Ukuran dataset sampel: {len(df_sample)} sampel\")\n",
    "print(f\"Distribusi kelas dalam sampel:\\n{y.value_counts(normalize=True).round(3)}\")\n",
    "\n",
    "# Split data sampel menjadi training dan validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nUkuran data training:\", len(X_train))\n",
    "print(\"Ukuran data validasi:\", len(X_val))\n",
    "\n",
    "# Identifikasi kolom kategorikal dan numerikal\n",
    "categorical_columns = []\n",
    "numerical_columns = []\n",
    "\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        categorical_columns.append(column)\n",
    "    else:\n",
    "        numerical_columns.append(column)\n",
    "\n",
    "# kolom kategorikal numerik\n",
    "numerical_categorical_columns = [\n",
    "    \"IsDomainIP\", \"HasObfuscation\", \"IsHTTPS\", \"HasTitle\", \"HasFavicon\",\n",
    "    \"IsResponsive\", \"HasExternalFormSubmit\", \"HasSocialNet\", \"HasSubmitButton\",\n",
    "    \"HasHiddenFields\", \"HasPasswordField\", \"Bank\", \"Pay\", \"Crypto\",\n",
    "    \"HasCopyrightInfo\", \"Robots\", \"HasDescription\"\n",
    "]\n",
    "\n",
    "# Update categorical_columns\n",
    "categorical_columns.extend(numerical_categorical_columns)\n",
    "numerical_columns = [col for col in numerical_columns if col not in numerical_categorical_columns]\n",
    "\n",
    "print(\"\\nJumlah kolom kategorikal:\", len(categorical_columns))\n",
    "print(\"Jumlah kolom numerikal:\", len(numerical_columns))\n",
    "\n",
    "# 1. Missing Value Handling\n",
    "print(\"\\nHandling missing values...\")\n",
    "missing_handler = MissingValueHandler(\n",
    "    categorical_columns=categorical_columns,\n",
    "    numerical_columns=numerical_columns,\n",
    "    strategy='advanced'\n",
    ")\n",
    "X_train_clean = missing_handler.fit_transform(X_train)\n",
    "X_val_clean = missing_handler.transform(X_val)\n",
    "\n",
    "# 2. Outlier Handling\n",
    "print(\"\\nHandling outliers...\")\n",
    "outlier_handler = OutlierHandler(\n",
    "    numerical_columns=numerical_columns,\n",
    "    method='zscore',\n",
    "    threshold=3,\n",
    "    strategy='clip'\n",
    ")\n",
    "X_train_clean = outlier_handler.fit_transform(X_train_clean)\n",
    "X_val_clean = outlier_handler.transform(X_val_clean)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "print(\"\\nApplying feature engineering...\")\n",
    "feature_engineer = URLWebsiteFeatureEngineer()\n",
    "X_train_featured = feature_engineer.fit_transform(X_train_clean)\n",
    "X_val_featured = feature_engineer.transform(X_val_clean)\n",
    "\n",
    "# Setelah Outlier Handling dan sebelum Feature Scaling\n",
    "print(\"\\nHandling infinite values...\")\n",
    "X_train_featured = handle_infinite_values(X_train_featured, numerical_columns)\n",
    "X_val_featured = handle_infinite_values(X_val_featured, numerical_columns)\n",
    "\n",
    "# 4. Feature Scaling\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = FeatureScaler(method='standard')\n",
    "X_train_scaled = scaler.fit_transform(X_train_featured)\n",
    "X_val_scaled = scaler.transform(X_val_featured)\n",
    "\n",
    "# 5. Feature Encoding\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "encoder = FeatureEncoder(method='label')\n",
    "X_train_encoded = encoder.fit_transform(X_train_scaled)\n",
    "X_val_encoded = encoder.transform(X_val_scaled)\n",
    "\n",
    "# 6. Handle Imbalanced Data\n",
    "print(\"\\nHandling class imbalance...\")\n",
    "imbalance_handler = ImbalanceHandler(method='smote', sampling_ratio=1.0)\n",
    "X_train_balanced, y_resampled = imbalance_handler.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# Training dan evaluasi model\n",
    "print(\"\\n=== Evaluasi Model dengan 30% Dataset ===\")\n",
    "\n",
    "# 1. KNN from scratch\n",
    "print(\"\\nTraining KNN from scratch...\")\n",
    "knn = KNNFromScratch(k=5, metric='euclidean')\n",
    "knn.fit(X_train_balanced, y_resampled)\n",
    "knn_pred = knn.predict(X_val_encoded)\n",
    "\n",
    "print(\"\\nKNN From Scratch Results:\")\n",
    "print(\"-----------------------\")\n",
    "print(classification_report(y_val, knn_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, knn_pred))\n",
    "\n",
    "# 2. Naive Bayes from scratch\n",
    "print(\"\\nTraining Naive Bayes from scratch...\")\n",
    "nb = GaussianNaiveBayesFromScratch()\n",
    "nb.fit(X_train_balanced, y_resampled)\n",
    "nb_pred = nb.predict(X_val_encoded)\n",
    "\n",
    "print(\"\\nNaive Bayes From Scratch Results:\")\n",
    "print(\"-------------------------------\")\n",
    "print(classification_report(y_val, nb_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, nb_pred))\n",
    "\n",
    "# 3. Perbandingan dengan implementasi scikit-learn\n",
    "print(\"\\n=== Perbandingan dengan Scikit-learn ===\")\n",
    "\n",
    "# KNN scikit-learn\n",
    "print(\"\\nTraining scikit-learn KNN...\")\n",
    "sklearn_knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "sklearn_knn.fit(X_train_balanced, y_resampled)\n",
    "sklearn_knn_pred = sklearn_knn.predict(X_val_encoded)\n",
    "\n",
    "print(\"\\nScikit-learn KNN Results:\")\n",
    "print(\"-----------------------\")\n",
    "print(classification_report(y_val, sklearn_knn_pred))\n",
    "\n",
    "# Naive Bayes scikit-learn\n",
    "print(\"\\nTraining scikit-learn Naive Bayes...\")\n",
    "sklearn_nb = GaussianNB()\n",
    "sklearn_nb.fit(X_train_balanced, y_resampled)\n",
    "sklearn_nb_pred = sklearn_nb.predict(X_val_encoded)\n",
    "\n",
    "print(\"\\nScikit-learn Naive Bayes Results:\")\n",
    "print(\"------------------------------\")\n",
    "print(classification_report(y_val, sklearn_nb_pred))\n",
    "\n",
    "# Menyimpan model terbaik\n",
    "accuracies = {\n",
    "    'KNN_scratch': accuracy_score(y_val, knn_pred),\n",
    "    'NB_scratch': accuracy_score(y_val, nb_pred),\n",
    "    'KNN_sklearn': accuracy_score(y_val, sklearn_knn_pred),\n",
    "    'NB_sklearn': accuracy_score(y_val, sklearn_nb_pred)\n",
    "}\n",
    "\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "print(f\"\\nModel terbaik: {best_model_name} dengan akurasi: {accuracies[best_model_name]:.4f}\")\n",
    "\n",
    "# Memilih model terbaik untuk disimpan\n",
    "best_model = None\n",
    "if best_model_name == 'KNN_scratch':\n",
    "    best_model = knn\n",
    "elif best_model_name == 'NB_scratch':\n",
    "    best_model = nb\n",
    "elif best_model_name == 'KNN_sklearn':\n",
    "    best_model = sklearn_knn\n",
    "else:\n",
    "    best_model = sklearn_nb\n",
    "\n",
    "# Menyimpan model terbaik\n",
    "filename = f'best_model_{best_model_name}.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\nModel terbaik telah disimpan sebagai '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for infinity values after cleaning...\n",
      "Infinity values found in X_train_featured after cleaning.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for infinity values after cleaning...\")\n",
    "if np.isinf(X_train_featured[scaler.numeric_columns]).any().any():\n",
    "    print(\"Infinity values found in X_train_featured after cleaning.\")\n",
    "else:\n",
    "    print(\"No infinity values found in X_train_featured after cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
